import os
import re
import time
import sys
from collections import defaultdict
from typing import List

# ================= ä¾èµ–å¯¼å…¥ =================
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.retrievers import BM25Retriever

# ================= é…ç½®åŒº =================
API_KEY = "sk-kjcKGNQPBajYm3kHjpl7Kg"
BASE_URL = "http://10.10.22.76:4000/v1"
LLM_MODEL = "Qwen2.5-32B-Instruct"  
EMBED_MODEL = "Qwen3-Embedding-8B"       

FILE_PATH = "2025å¹´ä¸Šæµ·å¤§å­¦æœ¬ç§‘ç”Ÿå­¦ç”Ÿæ‰‹å†Œ.pdf" 
VECTOR_DB_PATH = "shu_handbook_demo_final_index"

class SHUCampusBot:
    def __init__(self):
        print("\nğŸ”µ [ç³»ç»Ÿå¯åŠ¨] æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½æ•™åŠ¡å¼•æ“...")
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=API_KEY, openai_api_base=BASE_URL,
            model=EMBED_MODEL, check_embedding_ctx_length=False
        )
        self.llm = ChatOpenAI(
            api_key=API_KEY, base_url=BASE_URL, 
            model=LLM_MODEL, temperature=0.1 # è°ƒä½æ¸©åº¦ï¼Œè®©å·¥å…·ç±»ä»»åŠ¡æ›´å¿«æ›´å‡†
        )
        self.documents = []
        self.vector_retriever = None
        self.bm25_retriever = None
        self.entity_index = defaultdict(list)
        self.chat_history = [] 

    def clean_text(self, text):
        text = re.sub(r'--- PAGE \d+ ---', '', text)
        text = re.sub(r'ä¸Šæµ·å¤§å­¦æœ¬ç§‘ç”Ÿå­¦ç”Ÿæ‰‹å†Œ', '', text)
        return re.sub(r'\s+', ' ', text.replace('\n', ' ')).strip()

    def extract_entities(self, text: str) -> List[str]:
        entities = []
        rule_pattern = re.compile(r"(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾0-9]+æ¡)")
        entities.extend(rule_pattern.findall(text))
        keywords = ["è½¬ä¸“ä¸š", "ä¼‘å­¦", "å¤å­¦", "é€€å­¦", "ç»©ç‚¹", "å¹³å‡å­¦åˆ†ç»©ç‚¹", 
                   "è¿çºª", "ä½œå¼Š", "å¤„åˆ†", "å­¦ä½", "æ¯•ä¸šè®¾è®¡", "è¡¥è€ƒ", "é‡ä¿®",
                   "è€ƒå‹¤", "è¯·å‡", "ç¼“è€ƒ", "å…å¬", "è¾…ä¿®", "ç¤¾å›¢", "æŒ‡å¯¼æ•™å¸ˆ"]
        for kw in keywords:
            if kw in text: entities.append(kw)
        return list(set(entities))

    def initialize_data(self):
        # å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
        if os.path.exists(VECTOR_DB_PATH):
            print("ğŸ“‚ æ£€æµ‹åˆ°æœ¬åœ°ç´¢å¼•ç¼“å­˜ï¼Œæ­£åœ¨å¿«é€Ÿè£…è½½...")
            try:
                self.vector_retriever = FAISS.load_local(
                    VECTOR_DB_PATH, self.embeddings, allow_dangerous_deserialization=True
                ).as_retriever(search_kwargs={"k": 6})
            except:
                pass

        print("ğŸ“š æ­£åœ¨åŠ è½½ã€Šå­¦ç”Ÿæ‰‹å†Œã€‹å¹¶æ„å»ºå¤šè·¯å¬å›å›¾è°±...")
        loader = PyPDFLoader(FILE_PATH)
        raw_pages = loader.load()
        
        cleaned_docs = []
        for page in raw_pages:
            txt = self.clean_text(page.page_content)
            if len(txt) > 10:
                cleaned_docs.append(Document(page_content=txt, metadata=page.metadata))
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800, chunk_overlap=150, separators=["\n\n", "ç¬¬", "ã€‚", "ï¼"]
        )
        self.documents = text_splitter.split_documents(cleaned_docs)
        
        if not self.vector_retriever:
            vectorstore = FAISS.from_documents(self.documents, self.embeddings)
            vectorstore.save_local(VECTOR_DB_PATH)
            self.vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        
        try:
            from rank_bm25 import BM25Okapi
            self.bm25_retriever = BM25Retriever.from_documents(self.documents)
            self.bm25_retriever.k = 6
        except:
            pass

        for doc in self.documents:
            ents = self.extract_entities(doc.page_content)
            for ent in ents:
                self.entity_index[ent].append(doc)
        
        print(f"âœ… ç³»ç»Ÿå°±ç»ª! ç‰‡æ®µ:{len(self.documents)} å®ä½“:{len(self.entity_index)}")

    # --- å•ç‹¬å°è£…é‡å†™é€»è¾‘ï¼Œä¾›å‰ç«¯è°ƒç”¨ ---
    def rewrite_query(self, user_input):
        if not self.chat_history:
            return user_input # æ²¡æœ‰å†å²ï¼Œç›´æ¥è¿”å›åŸé—®é¢˜ï¼Œç§’å›ï¼
        
        # âš¡ï¸ ä¼˜åŒ– Promptï¼šå¼ºåˆ¶åªè¾“å‡ºç»“æœï¼Œä¸è¦åºŸè¯
        context_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢é‡å†™å·¥å…·ã€‚è¯·ç»“åˆå†å²å¯¹è¯ï¼Œå°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªå®Œæ•´çš„ã€ç‹¬ç«‹çš„æŸ¥è¯¢è¯­å¥ã€‚\n"
                       "è§„åˆ™ï¼š\n"
                       "1. è¡¥å…¨ç¼ºå¤±çš„ä¸»è¯­æˆ–å®¾è¯­ï¼ˆå¦‚å°†â€œå®ƒâ€æ›¿æ¢ä¸ºå…·ä½“çš„ç¤¾å›¢åï¼‰ã€‚\n"
                       "2. ä¸¥ç¦è¾“å‡ºâ€œå¥½çš„â€ã€â€œæ”¹å†™å¦‚ä¸‹â€ç­‰åºŸè¯ã€‚\n"
                       "3. ç›´æ¥è¾“å‡ºæ”¹å†™åçš„å¥å­ã€‚"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])
        context_chain = context_prompt | self.llm | StrOutputParser()
        return context_chain.invoke({
            "chat_history": self.chat_history,
            "question": user_input
        })

    def hybrid_retrieve(self, query: str, top_k=6) -> List[Document]:
        entities = self.extract_entities(query)
        graph_docs = []
        if entities:
            for ent in entities:
                if ent in self.entity_index:
                    for d in self.entity_index[ent]:
                        d.metadata['source'] = f'æ¡æ¬¾åŒ¹é…({ent})'
                        graph_docs.append(d)
        
        vec_docs = self.vector_retriever.invoke(query)
        for d in vec_docs: d.metadata['source'] = 'è¯­ä¹‰æœç´¢'
        
        bm25_docs = []
        if self.bm25_retriever:
            bm25_docs = self.bm25_retriever.invoke(query)
            for d in bm25_docs: d.metadata['source'] = 'å…³é”®è¯'
        
        all_docs = graph_docs + vec_docs + bm25_docs
        unique_docs = []
        seen = set()
        for doc in all_docs:
            sig = doc.page_content[:50]
            if sig not in seen:
                unique_docs.append(doc)
                seen.add(sig)
        return unique_docs[:top_k]

# ================= å¯åŠ¨ç¨‹åº =================
if __name__ == "__main__":
    bot = SHUCampusBot()
    bot.initialize_data()
    # å‘½ä»¤è¡Œæµ‹è¯•é€»è¾‘...ï¼ˆç•¥ï¼Œä¸»è¦çœ‹app.pyï¼‰