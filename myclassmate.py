import os
import time
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ================= æ ¸å¿ƒé…ç½® (å­¦æ ¡æœåŠ¡å™¨) =================
API_KEY = "sk-kjcKGNQPBajYm3kHjpl7Kg"
BASE_URL = "http://10.10.22.76:4000/v1"

# ä¸¤ä¸ªéƒ½ç”¨å­¦æ ¡æä¾›çš„æ¨¡å‹
LLM_MODEL = "Qwen2.5-32B-Instruct"       # ç”¨äºå¯¹è¯
EMBED_MODEL = "Qwen3-Embedding-8B"       # ç”¨äºå‘é‡åŒ–

FILE_PATH = "2025å¹´ä¸Šæµ·å¤§å­¦æœ¬ç§‘ç”Ÿå­¦ç”Ÿæ‰‹å†Œ.docx"
VECTOR_DB_PATH = "shu_handbook_server_index" # æœ¬åœ°ç¼“å­˜æ–‡ä»¶å¤¹å
BATCH_SIZE = 10  # ğŸŒŸå…³é”®è®¾ç½®ï¼šæ¯æ¬¡åªå‘ 10 æ®µç»™æœåŠ¡å™¨ï¼Œé˜²æ­¢å®ƒ 502
# =======================================================

def get_server_embeddings():
    """é…ç½®è¿æ¥å­¦æ ¡çš„ Embedding æ¨¡å‹"""
    return OpenAIEmbeddings(
        openai_api_key=API_KEY,
        openai_api_base=BASE_URL,
        model=EMBED_MODEL,
        check_embedding_ctx_length=False
    )

def init_vector_store():
    """æ™ºèƒ½åˆå§‹åŒ–ï¼šæœ‰ç¼“å­˜è¯»ç¼“å­˜ï¼Œæ²¡ç¼“å­˜å»è¿æœåŠ¡å™¨"""
    embeddings = get_server_embeddings()

    # 1. æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰ç¼“å­˜ 
    if os.path.exists(VECTOR_DB_PATH):
        print("ğŸ’¾ æ£€æµ‹åˆ°æœ¬åœ°å·²å­˜åœ¨å‘é‡åº“ï¼Œæ­£åœ¨ç›´æ¥åŠ è½½...")
        print("   (æœ¬æ¬¡å¯åŠ¨ä¸éœ€è¦è¿æ¥å­¦æ ¡ Embedding æ¨¡å‹ï¼Œé€Ÿåº¦æå¿«)")
        try:
            return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"âŒ æœ¬åœ°ç¼“å­˜æŸåï¼Œå‡†å¤‡é‡æ–°æ„å»º: {e}")

    # 2. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œå¼€å§‹é‡æ–°æ„å»º
    print(f"ğŸ“š æ­£åœ¨è¯»å–æ–‡æ¡£: {FILE_PATH} ...")
    if not os.path.exists(FILE_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {FILE_PATH}")

    loader = Docx2txtLoader(FILE_PATH)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]
    )
    splits = text_splitter.split_documents(docs)
    total = len(splits)
    print(f"âœ‚ï¸  æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {total} ä¸ªç‰‡æ®µã€‚")

    print(f"ğŸš€ å¼€å§‹è¿æ¥å­¦æ ¡æœåŠ¡å™¨ ({EMBED_MODEL}) æ„å»ºç´¢å¼•...")
    print(f"ğŸ›¡ï¸  å¯ç”¨å®‰å…¨æ¨¡å¼ï¼šæ¯æ‰¹å‘é€ {BATCH_SIZE} æ®µï¼Œé˜²æ­¢æœåŠ¡å™¨ 502...")

    vectorstore = None
    
    # ğŸŒŸ æ ¸å¿ƒä¼˜åŒ–ï¼šåˆ†æ‰¹å¾ªç¯å‘é€è¯·æ±‚
    for i in range(0, total, BATCH_SIZE):
        batch = splits[i : i + BATCH_SIZE]
        print(f"   æ­£åœ¨å¤„ç†è¿›åº¦: {i}/{total} ...")
        
        try:
            if vectorstore is None:
                vectorstore = FAISS.from_documents(batch, embeddings)
            else:
                vectorstore.add_documents(batch)
            # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé˜²æ­¢æœåŠ¡å™¨åˆ¤å®šæ”»å‡»
            time.sleep(0.5) 
        except Exception as e:
            print(f"\nâŒ åœ¨ç¬¬ {i} æ®µå¤„å‘ç”Ÿé”™è¯¯: {e}")
            print("ğŸ’¡ å»ºè®®ï¼šå¦‚æœæ˜¯ 502ï¼Œè¯·è”ç³»ç®¡ç†å‘˜é‡å¯æœåŠ¡ï¼›å¦‚æœæ˜¯ 429ï¼Œè¯·è°ƒå° BATCH_SIZEã€‚")
            raise e

    # 3. ä¿å­˜åˆ°æœ¬åœ°
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°ç¡¬ç›˜...")
    vectorstore.save_local(VECTOR_DB_PATH)
    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆå¹¶ä¿å­˜ï¼ä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥è·³è¿‡æ­¤æ­¥ã€‚")
    return vectorstore

def main():
    try:
        # 1. å‡†å¤‡å‘é‡åº“ (Embedding é˜¶æ®µ)
        vectorstore = init_vector_store()
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

        # 2. å‡†å¤‡å¯¹è¯æ¨¡å‹ (Chat é˜¶æ®µ)
        print(f"ğŸ”Œ æ­£åœ¨è¿æ¥å¯¹è¯æ¨¡å‹: {LLM_MODEL} ...")
        llm = ChatOpenAI(
            api_key=API_KEY, base_url=BASE_URL, 
            model=LLM_MODEL, temperature=0.1
        )

        # 3. æ„å»º RAG æ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šæµ·å¤§å­¦æ•™åŠ¡åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹æ–¹çš„ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œæ‰‹å†Œä¸­æœªæ‰¾åˆ°ç›¸å…³è§„å®šâ€ï¼Œä¸è¦ç¼–é€ ã€‚
        
        ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
        {context}
        
        ã€é—®é¢˜ã€‘ï¼š
        {question}
        
        å›ç­”ï¼š"""
        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("\nğŸ‰ ç³»ç»Ÿå°±ç»ªï¼å®Œå…¨è¿æ¥å­¦æ ¡æœåŠ¡å™¨ã€‚(è¾“å…¥ q é€€å‡º)")
        
        # 4. å¼€å§‹å¯¹è¯
        while True:
            query = input("\nğŸ™‹ æé—®: ")
            if query.lower() in ['q', 'exit']: break
            
            print("ğŸ¤– (æ­£åœ¨æ€è€ƒ)...")
            try:
                res = rag_chain.invoke(query)
                print(f"ğŸ“– å›ç­”:\n{res}")
            except Exception as e:
                print(f"âŒ Chatè¯·æ±‚å¤±è´¥: {e}")
                print("ğŸ’¡ å¯èƒ½æ˜¯å­¦æ ¡ Chat æ¨¡å‹æŒ‚äº† (502)ï¼Œè¯·å°è¯•é‡è¿ã€‚")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºç»ˆæ­¢: {e}")

if __name__ == "__main__":
    main()